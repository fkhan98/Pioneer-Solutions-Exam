{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coding_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0AxgFfwE4i4"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwYGYAXJ94ID"
      },
      "source": [
        "Sigmoid activation function used in the final layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axwz58hH5Lc"
      },
      "source": [
        "def sigmoid(x):\r\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6jO3Knb9-nJ"
      },
      "source": [
        "Calculating the derivative of Relu activation function\r\n",
        "We know Relu function takes the max value between 0 and Z so it has a gradient of 1 when Z > 0 & a gradient of 0 when Z <= 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3SF4ScuXgL6"
      },
      "source": [
        "def calculate_relu_grad(z1):\r\n",
        "  grad = []\r\n",
        "  for i in range(np.size(z1)):\r\n",
        "    if z1[i] > 0:\r\n",
        "      grad.append(1)\r\n",
        "    else:\r\n",
        "      grad.append(0)\r\n",
        "  return grad"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez03_OXj-Xrw"
      },
      "source": [
        "Initializing the Weight matrices to random values and biases to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idzcnlcMjND"
      },
      "source": [
        "def weight_initialize(nl_0=2,nl_1=4,nl_2=1):\r\n",
        "  w1 = np.random.rand(nl_1,nl_0)*0.02\r\n",
        "  b1 = np.zeros(nl_1,1)\r\n",
        "  w2 = np.random.rand(nl_2,nl_1)*0.02\r\n",
        "  b2 = np.zeros(nl_2,1)\r\n",
        "  parameters = {\r\n",
        "      \"w1\" : w1,\r\n",
        "      \"b1\" : b1,\r\n",
        "      \"w2\" : w2,\r\n",
        "      \"b2\" : b2\r\n",
        "  }\r\n",
        "  return parameters"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-cLWWeX-fmn"
      },
      "source": [
        "We pass X which is the input batch and the learnable parameters to the forward prop fucntion and that returns the \"z\" and \"a\" values for eavh layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBV8mfeqEMrv"
      },
      "source": [
        "def forward_prop(X,parameters):\r\n",
        "  w1 = parameters[\"w1\"]\r\n",
        "  b1 = parameters[\"b1\"]\r\n",
        "  w2 = parameters[\"w2\"]\r\n",
        "  b2 = parameters[\"b2\"]\r\n",
        "  z1 = np.dot(w1,X) + b1\r\n",
        "  a1 = np.maximum(0,z1)\r\n",
        "  z2 = np.dot(w2,a1) + b2\r\n",
        "  a2 = sigmoid(z2)\r\n",
        "  temp = {\r\n",
        "      \"z1\" : z1,\r\n",
        "      \"a1\" : a1,\r\n",
        "      \"z2\" : z2,\r\n",
        "      \"a2\" : a2\r\n",
        "  }\r\n",
        "  return temp\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSOdEGoo-2Ge"
      },
      "source": [
        "We pass the Labels Y, batch X, learnable parameters and temp which contains all the \"z\" and \"a\" values of each layer. This function returns the calculated gradient for a particular iteration which is used to udate the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u69VYV6JByj"
      },
      "source": [
        "def backward_prop(Y,X,temp,parameters):\r\n",
        "  #GATHERING SOME IMPORTANT VALUES\r\n",
        "  m = X.shape[1]\r\n",
        "  z1 = temp[\"z1\"]\r\n",
        "  a1 = temp[\"a1\"]\r\n",
        "  z2 = temp[\"z2\"]\r\n",
        "  a2 = temp[\"a2\"]\r\n",
        "  w1 = parameters[\"w1\"]\r\n",
        "  b1 = parameters[\"b1\"]\r\n",
        "  w2 = parameters[\"w2\"]\r\n",
        "  b2 = parameters[\"b2\"] \r\n",
        "  #BACK PROP STARTS HERE\r\n",
        "  dz_2 = a2 - y #nl_2,m\r\n",
        "  dw_2 = (1/m)*(np.dot(dz_2,a1.T)) #nl_2,nl_1\r\n",
        "  db_2 = (1/m)*np.sum(dz_2,axis = 1,keepdims=True) #nl_2,1\r\n",
        "  g_1 = calculate_relu_grad(z1)\r\n",
        "  dz_1 = np.multiply(np.dot(w2.T,dz_2),g_1)\r\n",
        "  dw_1 = (1/m)*(np.dot(dz_1,X.T))\r\n",
        "  db_1 = (1/m)*np.sum(dz_1,axis = 1,keepdims=True)\r\n",
        "\r\n",
        "  gradient = {\r\n",
        "      \"dw_2\" : dw_2,\r\n",
        "      \"db_2\" : db_2,\r\n",
        "      \"dw_1\" : dw_1,\r\n",
        "      \"db_1\" : db_1\r\n",
        "  }\r\n",
        "\r\n",
        "  return gradient"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxi5kNeFa_Xq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}